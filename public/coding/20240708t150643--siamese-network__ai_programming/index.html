<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<title>Siamese Network | bardman&#39;s website</title>
	<link rel="canonical" href="http://localhost:1313/">
	<link rel='alternate' type='application/rss+xml' title="bardman&#39;s website RSS" href='/index.xml'>
	<link rel='stylesheet' type='text/css' href='/style.css'>
	<link rel="icon" href="/favicon.ico">
	<meta name="description" content="What is Siamese Network Analogy to Siamese twins. Class of neural network that contain multiple identical networks with same parameters and weights.
Training Data The Siamese Network relies on a lot of training data. They are trained with two types of paired images, positive and negative samples.
Positive samples Two images from the same class that are paired together and have been labeled as similar
Negative samples Same as positive sample, but the images don&#39;t match.">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="robots" content="index, follow">
	<meta charset="utf-8">
</head>
<body>
<main>
<header><h1 id="tag_Siamese Network">Siamese Network</h1></header>
<article>


<div id="outline-container-headline-1" class="outline-2">
<h2 id="headline-1">
What is Siamese Network
</h2>
<div id="outline-text-headline-1" class="outline-text-2">
<p>Analogy to Siamese twins. Class of neural network that contain multiple identical networks with same parameters and weights.</p>
</div>
</div>
<div id="outline-container-headline-2" class="outline-2">
<h2 id="headline-2">
Training
</h2>
<div id="outline-text-headline-2" class="outline-text-2">
<div id="outline-container-headline-3" class="outline-3">
<h3 id="headline-3">
Data
</h3>
<div id="outline-text-headline-3" class="outline-text-3">
<p>The Siamese Network relies on a lot of training data. They are trained with two types of paired images, <code class="verbatim">positive and negative samples</code>.</p>
<div id="outline-container-headline-4" class="outline-4">
<h4 id="headline-4">
Positive samples
</h4>
<div id="outline-text-headline-4" class="outline-text-4">
<p>Two images from the same class that are paired together and have been labeled as similar</p>
</div>
</div>
<div id="outline-container-headline-5" class="outline-4">
<h4 id="headline-5">
Negative samples
</h4>
<div id="outline-text-headline-5" class="outline-text-4">
<p>Same as positive sample, but the images don&#39;t match.</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-6" class="outline-3">
<h3 id="headline-6">
CNN for Feature Extraction
</h3>
<div id="outline-text-headline-6" class="outline-text-3">
<p>Pictured below is the <a href="denote:20240703T133612">Convolutional Neural Network</a> that provides the feature vector of an image. This feature vector is used for training.</p>
<p><img src="Images/CNN_for_Feature_Extraction/org_20240709-105240_screenshot.png" alt="Images/CNN_for_Feature_Extraction/org_20240709-105240_screenshot.png" title="Images/CNN_for_Feature_Extraction/org_20240709-105240_screenshot.png" /></p>
<p>
After each image goes through the <span style="text-decoration: underline;">singular</span> CNN, we have to feature vectors. They get processed through a dense layer and a sigmoid function to get the final result. The result is a number, 0 or 1, symbolizing matching or not matching respectfully.</p>
<p><img src="Images/CNN_for_Feature_Extraction/org_20240709-110106_screenshot.png" alt="Images/CNN_for_Feature_Extraction/org_20240709-110106_screenshot.png" title="Images/CNN_for_Feature_Extraction/org_20240709-110106_screenshot.png" /></p>
<p>
The reason why they are called Siamese Network is because they both have their own &#34;body&#34; (feature vector), but share a &#34;head&#34; (dense layer to sigmoid process).</p>
</div>
</div>
<div id="outline-container-headline-7" class="outline-3">
<h3 id="headline-7">
Updating
</h3>
<div id="outline-text-headline-7" class="outline-text-3">
<p>Data goes &#34;backwards&#34; through the &#34;pipeline&#34;. First through loss function, and then comparison is made with dense layer and its parameters. The parameters get updated, and then move towards the convolutional layers for comparison.</p>
<p><img src="Images/CNN_for_Feature_Extraction/org_20240709-110546_screenshot.png" alt="Images/CNN_for_Feature_Extraction/org_20240709-110546_screenshot.png" title="Images/CNN_for_Feature_Extraction/org_20240709-110546_screenshot.png" /></p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-8" class="outline-2">
<h2 id="headline-8">
Data for Training Siamese Network
</h2>
<div id="outline-text-headline-8" class="outline-text-2">
<div id="outline-container-headline-9" class="outline-3">
<h3 id="headline-9">
<a href="denote:20240708T141825">One Shot Prediction</a>
</h3>
<div id="outline-text-headline-9" class="outline-text-3">
<p>Using the pre-trained model, we can now make predictions with a support set (e.g. 6-way 1-shot) that should be one-shot. This is pretty difficult.</p>
<p>
When the model then makes a prediction and assigns the query image a similarity score according to how similar it is compared to the images in the support set classes.</p>
</div>
</div>
<div id="outline-container-headline-10" class="outline-3">
<h3 id="headline-10">
Triplet Loss
</h3>
<div id="outline-text-headline-10" class="outline-text-3">
<p>Instead of training with many many images, the triplet loss similarity function uses 3 images per class. Each training sample is 3 random images.</p>
<div id="outline-container-headline-11" class="outline-4">
<h4 id="headline-11">
Data Preparation (3 Steps)
</h4>
<div id="outline-text-headline-11" class="outline-text-4">
<p>First, anchor image is randomly selected from the entirety of the data pool. Next, select a random image from the same class as a positive sample. Finally, exclude the anchor&#39;s data class and select another random image. This one will be used as the negative sample.</p>
</div>
</div>
<div id="outline-container-headline-12" class="outline-4">
<h4 id="headline-12">
Training
</h4>
<div id="outline-text-headline-12" class="outline-text-4">
<p>The image below shows how the triplets are processed by the same CNN, just like before. Their feature vectors are extracted and compared.</p>
<p><img src="Images/Data_for_Training_Siamese_Network/org_20240709-113957_screenshot.png" alt="Images/Data_for_Training_Siamese_Network/org_20240709-113957_screenshot.png" title="Images/Data_for_Training_Siamese_Network/org_20240709-113957_screenshot.png" /></p>
<p>
This is a visualization of the comparison, also known as the feature space</p>
<p><img src="Images/Data_for_Training_Siamese_Network/org_20240709-114223_screenshot.png" alt="Images/Data_for_Training_Siamese_Network/org_20240709-114223_screenshot.png" title="Images/Data_for_Training_Siamese_Network/org_20240709-114223_screenshot.png" /></p>
<p>
The distance between the feature vector d+ (positive sample) should be much closer than d- (negative sample). This comparison is the <span style="text-decoration: underline;">loss function</span>.</p>
</div>
</div>
<div id="outline-container-headline-13" class="outline-4">
<h4 id="headline-13">
Prediction
</h4>
<div id="outline-text-headline-13" class="outline-text-4">
<p>We can once again use the one-shot prediction as before to test the the neural network&#39;s accuracy. The result is different though. Because the result of the function is a distance vector, the prediction with the lowest distance is the one that wins due to the fact that it is closest to our query image.</p>
</div>
</div>
</div>
</div>
</div>
</div>


</article>
</main>

<footer>
        <a href="http://localhost:1313/">http://localhost:1313/</a><br><br><a href="/index.xml"><img src="/rss.svg" style="max-height:1.5em" alt="RSS Feed" title="Subscribe via RSS for updates."></a>
</footer>

</body>
</html>
